# OpenNebula Helm Chart Values
# See: https://github.com/pablodelarco/opennebula-helm

## Image configuration
image:
  repository: pablodelarco/opennebula
  tag: "latest"
  pullPolicy: IfNotPresent

## OpenNebula settings
opennebula:
  ## Admin password for oneadmin user
  ## Default: opennebula (override in production!)
  adminPassword: "opennebula"

## MariaDB subchart configuration
## ref: https://github.com/bitnami/charts/tree/main/bitnami/mariadb
mariadb:
  enabled: true
  architecture: standalone
  auth:
    database: opennebula
    username: oneadmin
    ## Password is auto-generated if not set
    # password: ""
  primary:
    persistence:
      enabled: true
      size: 8Gi

## External database configuration (when mariadb.enabled=false)
## Required fields when using external database
externalDatabase:
  host: ""
  port: 3306
  database: opennebula
  username: oneadmin
  ## Password for external database
  password: ""
  ## Or use existing secret (takes precedence over password)
  ## Secret must contain key 'mariadb-password'
  existingSecret: ""

## Persistence for OpenNebula data (/var/lib/one)
persistence:
  enabled: true
  size: 20Gi
  ## storageClass: ""  # Use cluster default
  accessMode: ReadWriteOnce

## Service configuration
service:
  type: ClusterIP
  ## Ports exposed
  ## oned: 2633, sunstone: 9869, fireedge: 2616
  ## oneflow: 2474, onegate: 5030

## FireEdge proxy configuration (handles __HOST__ replacement)
## Enabled by default - required for web UI to work correctly
fireedgeProxy:
  enabled: true

  ## Optional: Override auto-detected public URL
  ## Leave empty for auto-detection (recommended)
  ## Set only if you need a specific URL different from how users access it
  publicURL: ""

  ## Nginx image for sidecar
  image:
    repository: nginx
    tag: "1.27-alpine"

  ## Resources for nginx sidecar (lightweight)
  resources:
    limits:
      cpu: 100m
      memory: 64Mi
    requests:
      cpu: 10m
      memory: 16Mi

## Ingress configuration for FireEdge web UI
ingress:
  enabled: false
  className: ""
  annotations: {}
  hostname: opennebula.local
  ## TLS configuration
  tls:
    enabled: false
    secretName: ""  # Name of TLS secret

## SSH keys for hypervisor communication
## If not provided, keys will be generated and stored in a persistent secret
ssh:
  ## Auto-generate SSH keys when onedeploy is enabled
  ## Set to false to disable auto-generation (requires manual key config)
  autoGenerate: true

  ## Provide existing keys (base64 encoded) - overrides autoGenerate
  # privateKey: ""
  # publicKey: ""

## Resource limits and requests
## No defaults - set based on your environment
resources: {}
  # limits:
  #   cpu: 2
  #   memory: 4Gi
  # requests:
  #   cpu: 500m
  #   memory: 1Gi

## Pod security context
podSecurityContext: {}

## Container security context
securityContext: {}

## Node selector
nodeSelector: {}

## Tolerations
tolerations: []

## Affinity
affinity: {}

## ============================================================
## ONEDEPLOY INTEGRATION - Hypervisor Node Provisioning
## ============================================================
## This section mirrors the OneDeploy inventory format
## Users familiar with OneDeploy can use the same configuration
## Ref: https://github.com/OpenNebula/one-deploy

onedeploy:
  ## Enable automatic provisioning of hypervisor nodes
  ## When enabled, a Kubernetes Job will provision the specified hosts
  enabled: false

  ## --------------------------------------------------------
  ## SSH Host Key Handling (automatic)
  ## --------------------------------------------------------
  ## The OpenNebula frontend is configured with StrictHostKeyChecking=accept-new
  ## which automatically adds new hypervisor host keys to known_hosts on first
  ## connection. This eliminates the need for manual ssh-keyscan.
  ##
  ## For additional security, you can pre-populate known_hosts by providing
  ## the host keys in a ConfigMap (advanced use case, not required for most deployments).

  ## --------------------------------------------------------
  ## Global Variables (maps to all.vars in OneDeploy)
  ## --------------------------------------------------------
  vars:
    ## SSH user for connecting to nodes (must have sudo/root access)
    ansible_user: root

    ## OpenNebula version to install on nodes
    one_version: "7.0"

    ## Ensure hosts are registered with OpenNebula after provisioning
    ensure_hosts: true

    ## --------------------------------------------------------
    ## Virtual Networks (maps to vn in OneDeploy)
    ## Define networks that will be created in OpenNebula
    ## At minimum, you need ONE network for VMs to get IP addresses
    ## --------------------------------------------------------
    vn: {}
      ## IMPORTANT: Uncomment and configure at least one network!
      ##
      ## Example 1: Simple bridge network (most common)
      ## This creates a bridge on the host's physical interface
      # default:
      #   managed: true
      #   template:
      #     VN_MAD: bridge           # Network driver: bridge, 802.1Q, vxlan, ovswitch
      #     PHYDEV: eth0             # Physical interface on hypervisors
      #     BRIDGE: br0              # Bridge name to create
      #     AR:                      # Address Range - IPs for VMs
      #       TYPE: IP4
      #       IP: 192.168.1.100      # First IP to assign to VMs
      #       SIZE: 50               # Number of IPs (100-149 in this example)
      #     NETWORK_ADDRESS: 192.168.1.0
      #     NETWORK_MASK: 255.255.255.0
      #     GATEWAY: 192.168.1.1     # Router for internet access
      #     DNS: 8.8.8.8             # DNS server for VMs
      #
      ## Example 2: VLAN-tagged network
      # vlan100:
      #   managed: true
      #   template:
      #     VN_MAD: 802.1Q
      #     PHYDEV: eth0
      #     VLAN_ID: 100
      #     AR:
      #       TYPE: IP4
      #       IP: 10.100.0.10
      #       SIZE: 200
      #     GATEWAY: 10.100.0.1
      #     DNS: 10.100.0.1
      #
      ## Example 3: Tailscale/overlay network (no bridge, just routing)
      ## For Tailscale users - VMs get Tailscale IPs directly
      # tailscale_net:
      #   managed: true
      #   template:
      #     VN_MAD: dummy            # No physical network config needed
      #     AR:
      #       TYPE: IP4
      #       IP: 100.64.0.10        # Tailscale CGNAT range (or your assigned range)
      #       SIZE: 20
      #     # No gateway needed - Tailscale handles routing

    ## --------------------------------------------------------
    ## Datastores (maps to ds in OneDeploy)
    ## --------------------------------------------------------
    ds:
      ## Mode: ssh (local storage), shared (NFS), or ceph
      mode: ssh
      ## For shared mode, configure mounts:
      # config:
      #   mounts:
      #     - type: system
      #       path: /mnt/datastores/0
      #     - type: image
      #       path: /mnt/datastores/1

    ## --------------------------------------------------------
    ## NFS Mounts (maps to fstab in OneDeploy)
    ## Only needed for shared storage mode
    ## --------------------------------------------------------
    fstab: []
      ## Example NFS mount:
      # - src: "nfs-server:/var/lib/one/datastores"
      #   path: /mnt/datastores

    ## --------------------------------------------------------
    ## Optional Features
    ## --------------------------------------------------------
    features:
      prometheus: false
      ceph: false

  ## --------------------------------------------------------
  ## Hypervisor Nodes (maps to node.hosts in OneDeploy)
  ## Define the hosts that will be provisioned as hypervisors
  ## --------------------------------------------------------
  node:
    hosts: {}
      ## Example node configuration:
      # kvm-node-01:
      #   ansible_host: 192.168.1.101
      #   # virtualization: kvm  # default
      # kvm-node-02:
      #   ansible_host: 192.168.1.102
      # lxc-node-01:
      #   ansible_host: 192.168.1.103
      #   virtualization: lxc  # Override for LXC containers

  ## --------------------------------------------------------
  ## Bootstrap Configuration (Initial SSH Access)
  ## --------------------------------------------------------
  ## When SSH keys are auto-generated, provide your existing SSH key
  ## to inject the generated public key into hypervisors.
  ## Use: --set-file onedeploy.bootstrap.privateKey=~/.ssh/id_rsa
  bootstrap:
    ## SSH user for bootstrap (default: same as ansible_user or root)
    # user: root

    ## Your existing SSH private key that can access the hypervisors
    ## Used once to inject the auto-generated key, then discarded
    # privateKey: ""

  ## --------------------------------------------------------
  ## Provisioner Configuration (Helm-specific settings)
  ## --------------------------------------------------------
  provisioner:
    ## Docker image for the provisioner
    image:
      repository: pablodelarco/opennebula-provisioner
      tag: "latest"
      pullPolicy: IfNotPresent

    ## SSH configuration for connecting to hosts
    ssh:
      ## Private key for SSH access to nodes (base64 encoded)
      ## Generate: ssh-keygen -t ed25519 -f provisioner_key -N ""
      ## Encode: cat provisioner_key | base64 -w0
      privateKey: ""

      ## Or use an existing Kubernetes secret containing the SSH key
      ## Secret must have key 'id_rsa' with the private key
      existingSecret: ""

    ## Job configuration
    backoffLimit: 3
    ttlSecondsAfterFinished: 3600  # Clean up job after 1 hour

    ## Resources for the provisioner job
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi
